{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import keras\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Input,BatchNormalization,MaxPooling2D,Conv2DTranspose,concatenate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every image produces 25 non-repeating training images.\n",
      "Using images from 1 to 26 .\n",
      "patch shape: (650, 64, 64, 6)\n",
      "label shape: (650, 64, 64, 1)\n",
      "Class count:\n",
      "class 1  count 29704\n",
      "class 2  count 161965\n",
      "class 3  count 43262\n",
      "class 4  count 12705\n",
      "class 5  count 3725\n",
      "class 6  count 924669\n",
      "class 7  count 528721\n",
      "class 8  count 851917\n",
      "class 9  count 105732\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "size_pat=64 # pixel number of generated data in one dimension\n",
    "max_pixel_num=333 # each image is 333 x 333 pixels\n",
    "sample_1d = int(max_pixel_num/size_pat) # each image's sample number in one dimension\n",
    "sample_num_per_img = int(math.pow(sample_1d,2))\n",
    "print(\"Every image produces\",sample_num_per_img,\"non-repeating training images.\")\n",
    "class_to_train = 3 # [1,9]\n",
    "\n",
    "least = 1\n",
    "largest = 26\n",
    "print(\"Using images from\",least,\"to\",largest,\".\")\n",
    "\n",
    "# Generating data from image number \"least\" to \"largest\"\n",
    "total_sample_num = sample_num_per_img*(largest - least + 1)\n",
    "patch=np.ones((total_sample_num,size_pat,size_pat,6),float)\n",
    "label=np.zeros((total_sample_num,size_pat,size_pat,1),int)\n",
    "class_count = np.zeros(9,dtype=int)\n",
    "for i in range(least,largest+1):\n",
    "    s = 'I:/cis731projdataset/Image'+str(i)+'_allData.csv'\n",
    "    mydata = np.genfromtxt(s, delimiter=',')\n",
    "    for rs in range(sample_1d):# sample image row index\n",
    "        for cs in range(sample_1d):# sample image column index\n",
    "            p_index = (i-least)*sample_num_per_img+cs*sample_1d+rs\n",
    "            for ri in range(size_pat):# image pixel row index\n",
    "                for ci in range(size_pat):# image pixel column index\n",
    "                    data_index = (ci+cs*size_pat)*max_pixel_num+(rs*size_pat+ri)\n",
    "                    patch[p_index,ri,ci,:] = mydata[data_index,0:6]\n",
    "                    if(mydata[data_index,6] == 10):\n",
    "                        print(i)\n",
    "                        print(int(mydata[data_index,6]))\n",
    "                    class_count[int(mydata[data_index,6])-1] += 1\n",
    "                    if mydata[data_index,6]==class_to_train:\n",
    "                        label[p_index,ri,ci,0]=1\n",
    "print(\"patch shape:\", patch.shape)\n",
    "print(\"label shape:\", label.shape )\n",
    "print('Class count:')\n",
    "for l in range(len(class_count)):\n",
    "    print(\"class\",l+1,\" count\",class_count[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (520, 64, 64, 6) and (520, 64, 64, 1)\n",
      "Test set shape: (130, 64, 64, 6) and (130, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Allocate train set and test set\n",
    "fraction = 0.80\n",
    "train_num = int(total_sample_num * fraction)\n",
    "test_num = total_sample_num - train_num\n",
    "\n",
    "sample_index = random.sample(range(0,total_sample_num),total_sample_num)\n",
    "train_index = sample_index[:train_num]\n",
    "test_index = sample_index[train_num:]\n",
    "\n",
    "train_x=np.zeros((train_num,size_pat,size_pat,6),float)\n",
    "train_y=np.zeros((train_num,size_pat,size_pat,1),int)\n",
    "for i in range(train_num):\n",
    "    train_x[i] = patch[train_index[i]]\n",
    "    train_y[i] = label[train_index[i]]\n",
    "print(\"Train set shape:\",train_x.shape,\"and\",train_y.shape)\n",
    "\n",
    "test_x=np.zeros((test_num,size_pat,size_pat,6),float)\n",
    "test_y=np.zeros((test_num,size_pat,size_pat,1),int)\n",
    "for i in range(test_num):\n",
    "    test_x[i] = patch[test_index[i]]\n",
    "    test_y[i] = label[test_index[i]]\n",
    "print(\"Test set shape:\",test_x.shape,\"and\",test_y.shape)\n",
    "# train_x=patch[0:600]\n",
    "# train_y=label[0:600]\n",
    "# test_x=patch[600:]\n",
    "# test_y=label[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (?, 64, 64, 6)\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "# parameters:\n",
    "num_channel = 64 # if increase this, network reaches higher training accuracy faster and may overfitting\n",
    "size_kernel = (3,3)\n",
    "\n",
    "# The input tensor\n",
    "inputs = Input(shape=(size_pat,size_pat,6))\n",
    "print(\"input shape: \",np.shape(inputs))\n",
    "\n",
    "#layer1 64x64x6 -> 64x64x num_channel\n",
    "layer1 = Conv2D(num_channel, (3, 3),strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(inputs)\n",
    "\n",
    "\n",
    "#layer2 64x64x num_channel -> 32x32x num_channel\n",
    "layer2 = BatchNormalization(axis=-1, momentum=0.01)(layer1)\n",
    "layer2_c = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer2)\n",
    "\n",
    "layer2 = BatchNormalization(axis=-1, momentum=0.01)(layer2_c)\n",
    "layer2 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer2)\n",
    "layer2 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(layer2)\n",
    "#9\n",
    "\n",
    "#layer3 32x32x num_channel -> 16x16x num_channel\n",
    "layer3 = BatchNormalization(axis=-1, momentum=0.01)(layer2)\n",
    "layer3 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer3)\n",
    "\n",
    "layer3 = BatchNormalization(axis=-1, momentum=0.01)(layer3)\n",
    "layer3_c = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer3)\n",
    "\n",
    "layer3 = BatchNormalization(axis=-1, momentum=0.01)(layer3_c)\n",
    "layer3 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer3)\n",
    "\n",
    "layer3 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(layer3)\n",
    "#19\n",
    "\n",
    "#layer4 16x16x num_channel -> 8x8x num_channel\n",
    "# layer4 = BatchNormalization(axis=-1, momentum=0.01)(layer3)\n",
    "# layer4 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer4)\n",
    "\n",
    "# layer4 = BatchNormalization(axis=-1, momentum=0.01)(layer4)\n",
    "# layer4_c = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer4)\n",
    "\n",
    "# layer4 = BatchNormalization(axis=-1, momentum=0.01)(layer4_c)\n",
    "# layer4 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer4)\n",
    "\n",
    "# layer4 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(layer4)\n",
    "\n",
    "\n",
    "#layer4_1 8x8x num_channel -> 4x4x num_channel\n",
    "# layer4_1 = BatchNormalization(axis=-1, momentum=0.01)(layer4)\n",
    "# layer4_1 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='elu')(layer4_1)\n",
    "\n",
    "# layer4_1 = BatchNormalization(axis=-1, momentum=0.01)(layer4_1)\n",
    "# layer4_1_c = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='elu')(layer4_1)\n",
    "\n",
    "# layer4_1 = BatchNormalization(axis=-1, momentum=0.01)(layer4_1_c)\n",
    "# layer4_1 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='elu')(layer4_1)\n",
    "\n",
    "# layer4_1 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(layer4_1)\n",
    "\n",
    "\n",
    "#layer5_0 4x4x num_channel -> 8x8x num_channel\n",
    "# layer5_0 = BatchNormalization(axis=-1, momentum=0.01)(layer4_1) #layer4_1 if 64 -> 4\n",
    "# layer5_0 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='elu')(layer5_0)\n",
    "\n",
    "# layer5_0 = BatchNormalization(axis=-1, momentum=0.01)(layer5_0)\n",
    "# layer5_0 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='elu')(layer5_0)\n",
    "\n",
    "# layer5_0 = BatchNormalization(axis=-1, momentum=0.01)(layer5_0)\n",
    "# layer5_0 = Conv2DTranspose(num_channel, size_kernel,strides=(2,2),padding='same',data_format='channels_last',activation='elu')(layer5_0)\n",
    "\n",
    "\n",
    "\n",
    "#layer5 concat and 8x8x num_channel -> 16x16x num_channel\n",
    "#layer5 = concatenate([layer4_1_c,layer5_0],axis=-1) #comment this if 64 -> 8\n",
    "\n",
    "# layer5 = BatchNormalization(axis=-1, momentum=0.01)(layer4) #layer 4 if 64 -> 8\n",
    "# layer5 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer5)\n",
    "\n",
    "# layer5 = BatchNormalization(axis=-1, momentum=0.01)(layer5)\n",
    "# layer5 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer5)\n",
    "\n",
    "# layer5 = BatchNormalization(axis=-1, momentum=0.01)(layer5)\n",
    "# layer5 = Conv2DTranspose(num_channel, size_kernel,strides=(2,2),padding='same',data_format='channels_last',activation='relu')(layer5)\n",
    "\n",
    "\n",
    "#layer6 concat and 16x16x num_channel -> 32x32x num_channel\n",
    "#layer6 = concatenate([layer4_c,layer5],axis=-1)\n",
    "\n",
    "layer6 = BatchNormalization(axis=-1, momentum=0.01)(layer3)\n",
    "layer6 = Conv2D(int(num_channel*1.5), size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer6)\n",
    "\n",
    "layer6 = BatchNormalization(axis=-1, momentum=0.01)(layer6)\n",
    "layer6 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer6)\n",
    "\n",
    "layer6 = BatchNormalization(axis=-1, momentum=0.01)(layer6)\n",
    "layer6 = Conv2DTranspose(num_channel, size_kernel,strides=(2,2),padding='same',data_format='channels_last',activation='relu')(layer6)\n",
    "#28\n",
    "\n",
    "#layer7 concat and 32x32x num_channel -> 64x64x num_channel\n",
    "layer7 = concatenate([layer3_c,layer6],axis=-1)\n",
    "\n",
    "layer7 = BatchNormalization(axis=-1, momentum=0.01)(layer2) #\n",
    "layer7 = Conv2D(int(num_channel*1.5), size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer7)\n",
    "\n",
    "layer7 = BatchNormalization(axis=-1, momentum=0.01)(layer7)\n",
    "layer7 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer7)\n",
    "\n",
    "layer7 = BatchNormalization(axis=-1, momentum=0.01)(layer7)\n",
    "layer7 = Conv2DTranspose(num_channel, size_kernel,strides=(2,2),padding='same',data_format='channels_last',activation='relu')(layer7)\n",
    "#38\n",
    "\n",
    "#output layer8 concat and convout\n",
    "layer8 = concatenate([layer2_c,layer7],axis=-1)\n",
    "\n",
    "layer8 = BatchNormalization(axis=-1, momentum=0.01)(layer8)\n",
    "layer8 = Conv2D(int(num_channel*1.5), size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer8)\n",
    "\n",
    "layer8 = BatchNormalization(axis=-1, momentum=0.01)(layer8)\n",
    "layer8 = Conv2D(num_channel, size_kernel,strides=(1, 1),padding='same',data_format='channels_last',activation='relu')(layer8)\n",
    "\n",
    "layer8 = Conv2D(1, (1, 1),strides=(1,1),padding='same',data_format='channels_last',activation='sigmoid')(layer8)\n",
    "#46\n",
    "model = Model(inputs = inputs, outputs = layer8)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running at:  2017-12-09 04:52:31\n",
      "Train on 364 samples, validate on 156 samples\n",
      "Epoch 1/100\n",
      "364/364 [==============================] - 22s - loss: 0.3230 - acc: 0.9186 - val_loss: 0.1891 - val_acc: 0.9719\n",
      "Epoch 2/100\n",
      "364/364 [==============================] - 11s - loss: 0.1350 - acc: 0.9831 - val_loss: 0.1009 - val_acc: 0.9827\n",
      "Epoch 3/100\n",
      "288/364 [======================>.......] - ETA: 2s - loss: 0.0852 - acc: 0.9849"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fdc8b7c72a22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start running at: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "print(\"Start running at: \", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "start = time.clock()\n",
    "model.fit(train_x, train_y,batch_size=32, validation_split=0.3, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction=model.predict(test_x)\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used(s):\",elapsed)\n",
    "eva = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Accuracy on test set: \",eva[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for producing image\n",
    "# def produce_img(name,data,boundary):\n",
    "#     image_data=[0 for i in range(size_pat * size_pat)]\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(len(data[0])):\n",
    "#             if (data[i][j][0] > boundary):\n",
    "#                 image_data[i*size_pat+j] = 1\n",
    "#     img = Image.new('1', (size_pat, size_pat))\n",
    "#     img.putdata(image_data)\n",
    "#     img.save(name)\n",
    "    \n",
    "# Function for producing image\n",
    "def produce_img(name,data):\n",
    "    image_data=[0 for i in range(size_pat * size_pat)]\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            image_data[i*size_pat+j] = data[i][j][0] * 256\n",
    "    img = Image.new('L', (size_pat, size_pat))\n",
    "    img.putdata(image_data)\n",
    "    img.save(name)\n",
    "    \n",
    "# from PIL import Image\n",
    "# import random\n",
    "# image_data=[random.randint(0,60) for i in range(128 * 128)]\n",
    "# img = Image.new('L', (128, 128))\n",
    "# img.putdata(image_data)\n",
    "# img.save('11.jpg')\n",
    "    \n",
    "\n",
    "# Print out miss count and right count for 1s\n",
    "def infor(prediction,label):\n",
    "    all_count = 0\n",
    "    miss_count = 0\n",
    "    right_count = 0\n",
    "    not_good_count = 0\n",
    "    for i in range(len(prediction)):\n",
    "        for j in range(len(prediction[0])):\n",
    "            if label[i][j] == 1:\n",
    "                all_count += 1\n",
    "                if prediction[i][j] >= 0.9:\n",
    "                    right_count += 1\n",
    "                elif prediction[i][j] >= 0.4:\n",
    "                    not_good_count += 1\n",
    "                else:\n",
    "                    miss_count +=1\n",
    "    print('Number of 1s:{0:^7}, right count:{1:^7}, not good count:{2:^7}, miss count:{3:^7}'.\n",
    "          format(all_count,right_count,not_good_count,miss_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    sl = 'label_'+str(i)+'.jpg'\n",
    "    sp = 'prediction_'+str(i)+'.jpg'\n",
    "    produce_img(sl,test_y[i])\n",
    "    produce_img(sp,prediction[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_x)):\n",
    "    infor(prediction[i],test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_x = train_x[50:100]\n",
    "train_test_y=train_y[50:100]\n",
    "eva = model.evaluate(train_test_x, train_test_y, verbose=0)\n",
    "print(\"Accuracy on part of train set: \",eva[1])\n",
    "prediction=model.predict(train_test_x)\n",
    "for i in range(len(train_test_x)):\n",
    "    sl = 'testlabel_'+str(i)+'.jpg'\n",
    "    sp = 'test_prediction_'+str(i)+'.jpg'\n",
    "    produce_img(sl,train_test_y[i])\n",
    "    produce_img(sp,prediction[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
